# -*- coding: utf-8 -*-
"""customer_personality.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ibTnkeSl3aK9n4irTJdCwaIADivvjupo

# **Clustering customer data**
"""

import pandas as pd
import numpy as np
import datetime
from datetime import date
import matplotlib
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from sklearn.preprocessing import StandardScaler, normalize
from sklearn import metrics
from sklearn.mixture import GaussianMixture
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
import warnings
warnings.filterwarnings('ignore')
data=pd.read_csv('marketing_campaign.csv',delimiter='\t')

data.head()

list(data.columns)

data.info()

data['Dt_Customer'] = pd.to_datetime(data['Dt_Customer'])
data.head(15)

data.describe().T

data.drop(['Z_CostContact', 'Z_Revenue'], axis=1, inplace=True)

data.isna().sum()

data = data[data['Income'].notnull()]
data.reset_index(drop=True, inplace=True)
data.head(15)

data.describe(exclude='number')

data['Year'] = data['Dt_Customer'].apply(lambda row: row.year)
data.head(15)

sns.countplot(data['Year'])

data['Age'] = data['Year'] - data['Year_Birth']
data

"""# **Data Visualization**

Dropping all non-numerical data
"""

vis_data = data.copy()
vis_data.drop([
    'ID',
    'Year_Birth',
    'Education',
    'Marital_Status',
    'Dt_Customer',
    'Year'
],axis=1, inplace=True)
vis_data.head(15)

sns.kdeplot(data['Income'])

"""Eliminating outliers (above 200000)"""

outlier_idx = vis_data[vis_data['Income'] > 200000].index
vis_data.drop(outlier_idx, inplace=True)

sns.kdeplot(vis_data['Income'])

sns.kdeplot(data['Age'])

"""Eliminating outliers in age (above 90 years)"""

outlier_age = vis_data.loc[vis_data['Age'] > 90].index
vis_data.drop(outlier_age, inplace=True)
vis_data.reset_index(drop=True, inplace=True)

sns.kdeplot(vis_data['Age'])

sns.countplot(vis_data['Kidhome'])

sns.countplot(vis_data['Teenhome'])

sns.countplot(vis_data['NumDealsPurchases'])

"""combine Kidhome and Teenhome whether it's having a kid or a Teen"""

vis_data['Kidhome'] = vis_data['Kidhome'].apply(lambda row: 1 if row >= 1 else 0)
vis_data['Teenhome'] = vis_data['Teenhome'].apply(lambda row: 1 if row >= 1 else 0)

vis_data.head(15)

from sklearn.manifold import TSNE, LocallyLinearEmbedding, MDS, Isomap
from sklearn.preprocessing import StandardScaler

c_al_data = vis_data.copy()
c_al_data.head(15)

num_col = ['Income', 'Age', 'NumDealsPurchases', 'NumWebVisitsMonth']
cat_col = ['Kidhome', 'Teenhome', 'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response', 'Complain']

c_al_data['Accepted'] = c_al_data[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']].sum(axis=1) > 0
c_al_data['Accepted'] = c_al_data['Accepted'].apply(lambda row: 1 if row else 0)

c_al_data.drop(['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response'], axis=1, inplace=True)
c_al_data.head(15)

"""StandardScaler() will normalize each column of c_al_data,so that each column will have μ = 0 and σ = 1."""

c_al_data = pd.DataFrame(StandardScaler().fit_transform(c_al_data), columns=c_al_data.columns)
for i in c_al_data.columns:
    c_al_data[i].astype(dtype=float)

c_al_data.info()

"""Principal component analysis aims at reducing a large set of variables to a small set that still contains most of the information in the large set. The technique of principal component analysis enables us to create and use a reduced set of variables, which are called principal factors."""

from sklearn.decomposition import PCA

pca = PCA(n_components=10)

pca_data = pd.DataFrame(pca.fit_transform(c_al_data))

plt.plot(pca.explained_variance_ratio_.cumsum())

c_al_data = pd.concat([c_al_data, pca_data], axis=1)

c_al_data

"""The k-means clustering method is an unsupervised machine learning technique used to identify clusters of data objects in a dataset.

here t-Distributed Stochastic Neighbor Embedding (t-SNE) is used for visualizing higher-dimensional data.

1.   Age and income clustering
"""

random.seed(500)

cluster_data = c_al_data.copy()[[ 'Income','Age']]

cluster_centers, _ = kmeans(cluster_data, 5)

cluster_data['cluster_labels'], _ = vq(cluster_data, cluster_centers)

tsne = TSNE(learning_rate=100)

tsne_results = cluster_data.copy()

tsne_results[['TSNE1', 'TSNE2']] = pd.DataFrame(tsne.fit_transform(c_al_data), columns=['TSNE1', 'TSNE2'])

plt.figure(figsize=(10,10))

sns.scatterplot(x='TSNE1', y='TSNE2', hue='cluster_labels', cmap=sns.color_palette(), data=tsne_results)

"""
2. Income , teens and kids clustering

"""

random.seed(500)

cluster_data = c_al_data.copy()[[ 'Income','NumWebVisitsMonth','NumWebPurchases', 'Age']]

cluster_centers, _ = kmeans(cluster_data, 5)

cluster_data['cluster_labels'], _ = vq(cluster_data, cluster_centers)

tsne = TSNE(learning_rate=100)

tsne_results = cluster_data.copy()

tsne_results[['TSNE1', 'TSNE2']] = pd.DataFrame(tsne.fit_transform(c_al_data), columns=['TSNE1', 'TSNE2'])

plt.figure(figsize=(10,10))

sns.scatterplot(x='TSNE1', y='TSNE2', hue='cluster_labels', cmap=sns.color_palette(), data=tsne_results)

"""
3.   Income,kidhome,teenhome,age, complain , accepted clustering

"""

random.seed(500)

cluster_data = c_al_data.copy()[[ 'Income','Kidhome','Teenhome', 'Age','Complain','Accepted']]

cluster_centers, _ = kmeans(cluster_data, 5)

cluster_data['cluster_labels'], _ = vq(cluster_data, cluster_centers)

tsne = TSNE(learning_rate=100)

tsne_results = cluster_data.copy()

tsne_results[['TSNE1', 'TSNE2']] = pd.DataFrame(tsne.fit_transform(c_al_data), columns=['TSNE1', 'TSNE2'])

plt.figure(figsize=(10,10))

sns.scatterplot(x='TSNE1', y='TSNE2', hue='cluster_labels', cmap=sns.color_palette(), data=tsne_results)

"""Conclusion:




features of Income,kidhome,teenhome,age, complain , accepted make a better performing model with more distinct groups for targeted adds offers and discounts than other combinations

"""